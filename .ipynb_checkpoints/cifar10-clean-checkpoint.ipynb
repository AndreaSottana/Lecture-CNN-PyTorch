{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision  # useful for computer vision stuff\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm # Gives us a cool progress bar when training :)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "NUM_EPOCHS = 1\n",
    "LOG_RATE = 10\n",
    "\n",
    "NUM_IMGS_TO_SHOW = 1000\n",
    "MODEL_SAVE_PATH = './cifar_net.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data\n",
    "Currently downloading but should leave it a smaller set on the repo instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the dataset\n",
    "It's a good idea to have a look at your data to see if everything seems good.\n",
    "\n",
    "In PyTorch iamges are represented 'channel-first' i.e. (Channels, Height, Width) or CHW\n",
    "\n",
    "On the other hand, Tensorflow uses 'channel-last' i.e. (Height, Width, Channels) HWC\n",
    "\n",
    "pyplot imshow expects channel-last therefore we need to transpose our image from CHW to HWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='data', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Root location: data\n",
       "    Split: Train"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1211e2650>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=os.cpu_count())\n",
    "trainLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def introimg(image):\n",
    "    npimg = image.numpy()\n",
    "    npimg = np.transpose(npimg, (1,2,0))\n",
    "    plt.imshow(npimg)\n",
    "    plt.show(mpimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our CNN model\n",
    "[Conv2d](https://pytorch.org/docs/stable/nn.html#conv2d) (in_channels, out_channels, kernel_size)\n",
    "\n",
    "Remember: out_channels = number of filters as by definition to be able to carry out the dot product \n",
    "\n",
    "[MaxPool2d](https://pytorch.org/docs/stable/nn.html#maxpool2d) (kernel_size, stride)\n",
    "\n",
    "[Linear](https://pytorch.org/docs/stable/nn.html#linear)\n",
    "(in_features, out_features)\n",
    "\n",
    "\n",
    "Note that x.view reshapes our tensor. In this example we want to reshape the output of our 2nd max pooling (after our conv2 layer) which has an output shape of (n, 16, 5, 5) into (1, 16*5*5) = (n, 400) as the fully connected 'linear' layer expects a vector shape. Where 'n' is our batch_size.\n",
    "\n",
    "The -1 means to automatically fill the remaining dimension which in this case would get filled to 'n'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the model summary if we feed in an RGB image of size (3, 32, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up our optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
